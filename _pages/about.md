---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- ## About Me -->
I am a PhD student at [The Hong Kong University of Science and Technology](https://hkust.edu.hk), [Department of Computer Science and Engineering](https://cse.hkust.edu.hk/).
I am fortunate to be advised by [Junxian He](https://jxhe.github.io/). 
I am generally interested in natural language processing and machine learning and currently working on model merging.

## üìù Publications
Most recent publications on [Google Scholar](https://scholar.google.com/citations?user=HqF5d38AAAAJ&hl=en).  
\* denotes co-first authors, $^\dagger$ denotes corresponding author/main advisor

<!--compression-->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='_pages/images/compression.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Compression Represents Intelligence Linearly**

Yuzhen Huang\*, *<ins>Jinghan Zhang</ins>*\*, Zifei Shan, Junxian He$^\dagger$  

<p><a href="https://arxiv.org/abs/2404.09937">paper</a> | <a href="https://huggingface.co/spaces/hkust-nlp/LLM_Compression_Leaderboard">leaderboard</a> | <a href="https://huggingface.co/datasets/hkust-nlp/llm-compression">dataset</a> | <a href="https://github.com/hkust-nlp/llm-compression-intelligence"><img src="https://img.shields.io/github/stars/hkust-nlp/llm-compression-intelligence?style=social" alt="" /></a></p>

- **Abstract**: In this paper, we study the relationship between compression rate and intelligence of LLMs.
</div>
</div>

<!--pem merging-->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='_pages/images/neurips2023merge_finalv-1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Composing Parameter-Efficient Modules with Arithmetic Operations**

*<ins>Jinghan Zhang</ins>*, Shiqi Chen, Junteng Liu, Junxian He$^\dagger$  

<p><a href="https://arxiv.org/abs/2306.14870">paper</a> | <a href="https://github.com/hkust-nlp/PEM_composition"><img src="https://img.shields.io/github/stars/hkust-nlp/PEM_composition?style=social" alt="" /></a></p>

- **Abstract**: In this paper, we study model merging on parameter-efficient modules like LoRA and (IA)^3.
</div>
</div>

<!--ceval-->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023 D&B</div><img src='_pages/images/ceval.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models**  

Yuzhen Huang\*, Yuzhuo Bai\*, Zhihao Zhu, Junlei Zhang, *<ins>Jinghan Zhang</ins>*, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, Junxian He$^\dagger$  

<p><a href="https://arxiv.org/abs/2305.08322">paper</a> | <a href="https://cevalbenchmark.com">website</a> | <a href="https://huggingface.co/datasets/ceval/ceval-exam">dataset</a> | <a href="https://github.com/hkust-nlp/ceval"><img src="https://img.shields.io/github/stars/hkust-nlp/ceval?style=social" alt="" /></a></p>

</div>
</div>

<!--felm-->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023 D&B</div><img src='_pages/images/felm_examples.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**FELM: Benchmarking Factuality Evaluation of Large Language Models**  

Shiqi Chen, Yiran Zhao, *<ins>Jinghan Zhang</ins>*, I-Chun Chern, Siyang Gao, Pengfei Liu, Junxian He$^\dagger$  

<p><a href="https://arxiv.org/abs/2310.00741">paper</a> | <a href="https://hkust-nlp.github.io/felm/">website</a> | <a href="https://huggingface.co/datasets/hkust-nlp/felm">dataset</a> | <a href="https://github.com/hkust-nlp/felm"><img src="https://img.shields.io/github/stars/hkust-nlp/felm?style=social" alt="" /></a></p>

</div>
</div>

<!-- **Composing Parameter-Efficient Modules with Arithmetic Operations**  
*<ins>Jinghan Zhang</ins>*, Shiqi Chen, Junteng Liu, Junxian He$^\dagger$  
NeurIPS 2023. [[arxiv]](https://arxiv.org/abs/2306.14870) [[github]](https://github.com/hkust-nlp/PEM_composition)

**C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models**  
Yuzhen Huang\*, Yuzhuo Bai\*, Zhihao Zhu, Junlei Zhang, *<ins>Jinghan Zhang</ins>*, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, Junxian He$^\dagger$  
NeurIPS 2023 (Datasets and Benchmarks track). [[arxiv]](https://arxiv.org/abs/2305.08322) [[github]](https://github.com/hkust-nlp/ceval) [[website]](https://cevalbenchmark.com) [[dataset]](https://huggingface.co/datasets/ceval/ceval-exam)

**FELM: Benchmarking Factuality Evaluation of Large Language Models**  
Shiqi Chen, Yiran Zhao, *<ins>Jinghan Zhang</ins>*, I-Chun Chern, Siyang Gao, Pengfei Liu, Junxian He$^\dagger$  
NeurIPS 2023 (Datasets and Benchmarks track). [[arxiv]](https://arxiv.org/abs/2310.00741) [[github]](https://github.com/hkust-nlp/felm) [[website]](https://hkust-nlp.github.io/felm/) [[dataset]](https://huggingface.co/datasets/hkust-nlp/felm)
-->

## üéñ Awards
- *2023.06* Outstanding Undergraduate Thesis in SEU (top 3%)  
- *2021.12* National Scholarship 

## üìñ Education
- *2024.02 - now* PhD student, Department of CSE, [HKUST](https://hkust.edu.hk), Hong Kong SAR, China.
- *2023.11 - 2024.01* Research Assistant, Department of CSE, [HKUST](https://hkust.edu.hk), Hong Kong SAR, China.
- *2019.09 - 2023.06* Undergraduate, Artificial Intelligence, [Southeast University](https://www.seu.edu.cn/), Nanjing, China.

<!--## üßëüèª‚Äçüíª Contact
Happy to chat about any topics :)
-->

<!-- Calendly badge widget begin -->
<link href="https://assets.calendly.com/assets/external/widget.css" rel="stylesheet">
<script src="https://assets.calendly.com/assets/external/widget.js" type="text/javascript" async></script>
<script type="text/javascript">window.onload = function() { Calendly.initBadgeWidget({ url: 'https://calendly.com/zhangcharlotte84/15min', text: 'Happy to chat about any topics :)', color: '#0069ff', textColor: '#ffffff', branding: undefined }); }</script>
<!-- Calendly badge widget end -->
